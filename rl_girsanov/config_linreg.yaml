# An example of config file used to train tire curves
# This file contains parameters for the neural network models for
# the front and rear tire as well as parameters for the optimization process

# The path for the dataset or the logs run for training the model
# train_data: tire_force_dataset.pkl

# # Prefix of the output file (front or rear) will be added at the end
# # of that file name to distinguish between a front or rear learned curve
# output_file: nn_curve

# Model parameters
model:
  # Number of observation
  n_y: 1
  # Number of control inputs
  n_u: 1
  # Number of hidden variables (true state of the system)
  n_x: 1
  # # Bounds constraints setting for the states
  # # Slack variables are created in lieu of the states --> Only for bounds constraints
  # # These are the hidden states for which bound constraints are imposed
  # state_constr:
  #   # It represents the indexes of the constrained state variables
  #   state_id: [0]
  #   # Penalty term for the slack variables
  #   state_penalty: 1.
  #   # slack constraints
  #   # Set of min max values for the slack id above
  #   state_bound: [[-1, 1]] #.inf, -.inf
  #   # Enforce bound constraints via proximal ?
  #   # Doing so augment the number of variables of the problem by the number of
  #   # slack constraints
  #   slack_proximal: True
  # # input constraints
  # input_constr:
  #   input_id: [0]
  #   input_bound: [[-1, 1]]
  # Specified if ts is fixed
  fixed_ts: False
  # Number of sampling for fixed initial cond and sequence of control
  num_particles: 1000
  # Define the parameters for the SDE solver
  sde_solver:
    init_step: 0.01
    max_steps: 2046
    specific_sde: False
    stepsize_controller:
      name: ConstantStepSize
      params:
        compile_steps: null
    adjoint:
      name: RecursiveCheckpointAdjoint
      # params:
    solver:
      name: ReversibleHeun
      # params
    # A value just below the time step for ConstantStepSize
    # and RecursiveCheckpointAdjoint
    brownian_tol: 0.01

apg_mpc:
  # The intial step size in case no linsearh arguments are provided
  stepsize: 0.1

  # The maximum number of gradient updates
  max_iter: 1

  # The adaptive coefficient to scale the momentum. nill values mean
  # that it is not used and rather beta_k = k /(k+3) is used as classical acceleration momentum
  # This value should be between 0 and 1
  moment_scale: null

  # The initial momentum.
  beta_init: 0.25

  # The stopping criteria of the algorithm based on gradient norm
  tol: 1.0e-4

  linesearch:
    max_stepsize: 0.5 # The maximum admissible step size
    coef: 0.1 # The agressiveness coefficient. The smaller the larger step size in the optimization
    decrease_factor: 0.4 # The decrease factor when performing the armijo linesearch
    increase_factor: 1.2 # The increase factor at each new gradient descent iteration
    # # The reset strategy at each iteration
    # # "conservative": re-use previous stepsize, producing a non increasing sequence of stepsizes. Slow convergence.
    # # "increase": attempt to re-use previous stepsize multiplied by increase_factor. Cheap and efficient heuristic.
    reset_option: increase # or conservative
    maxls: 4 # Maximum number of iterations during the line search




# Loss function parameters
# Parameters to weight objective in the loss function
loss:
  kl: 1.0 # Importance of loss on the kl divergence
  logprob: 1.0 # Importance on the weight of the logprob
  noise: 0 # How important it is to maximize noise while fitting the data
  horizon: 10 # Number of ahead steps for loss computation

# Random number generator. It can be a single value or a list of value
# if the training should be done under different neural network initialization
seed: 0

# Optimizer parameters
optimizer:
  - name: scale_by_adam
    # params:
    #   b1: 0.999
    #   b2: 0.9999
  - name: add_decayed_weights
    params:
      weight_decay: 0.001
  - name: exponential_decay
    scheduler: True
    params:
      init_value: -0.01 # Initial learning rate (Negative value for minimization)
      transition_steps: 100000 # Basically the maximum number of gradient steps
      decay_rate: 1. # Typically (end_value / init_value) if end_value is expected at end
  - name: adaptive_grad_clip
    params:
      clipping: 0.01

# Training parameters
training:
  train_batch: 128 # The mini batch size for the training dataset
  test_batch: 128 # The mini batch size for the training dataset
  nepochs: 500 # The number of epochs (full pass over the training dataset)
  data_split_rate: 100 # The frequency (in terms of nepochs) at which the data is reshuffled
  train_num_batch_eval: 15 # The number of batches when evaluating loss for display only
  patience: 15 # The number of epochs after which to stop the learningif no improvement in solution
  display_losses: True # Display the losses as the training evolves
  no_improvement_bound: 0.001 # The bound to detect no improvement in the loss
  test_freq: 50 # Number of gradient steps after which to evaluate and display losses
  save_freq: 500 # Number of gradient steps after which to save the current losses+nn
  ratio_test: 0.1 # The ratio number of data set used for evaluation test loss (only for printing)
  ratio_train: 0.1 # The ratio number of data set used for evaluation train loss (only for printing)
