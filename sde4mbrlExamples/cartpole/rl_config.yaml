##########################################################################
# Configuration file for training policies for the learned dynamics models

# # Cartpole environment extra args
# env_extra_args:
#   max_steps: 200

# SDE extra arguments
sde_extra_args:
  num_particles : 10
  use_gpu : True
  jax_gpu_mem_frac : 0.4

# Gaussian ensemble extra arguments
GE_extra_args:
  num_particles : 1
  use_gpu : True

# Seed numbers
seeds : [5, 25, 105, 305, 1005]

# Trainer configuration dictionary
cfg_trainer_ppo:
  timesteps : 600000 # Training time step, 700000
  headless : True
  disable_progressbar : False

# Extra parameters to the ppo -> PPO_DEFAULT_CONFIG: ppo.py
extra_alg_dict_ppo:
  rollouts: 2048 # number of steps per environment per update

# Value and Policy networks initialization parameters
pol_val_init_params_ppo:
  method_name : normal_
  mean : 0.0
  std : 0.1

################################################################################
############################# Generate reward data #############################

model_names:
  - groundtruth_PPO
  - cartpole_bb_random_sde_PPO
  - cartpole_bb_learned_sde_PPO
  - cartpole_bb_learned_si_sde_PPO
  - gaussian_mlp_ensemble_cartpole_random_PPO
  - gaussian_mlp_ensemble_cartpole_learned_PPO
recompute_all: False
# outfile: 'rewards_data.pkl'
outfile: 'rewards_data_v2.pkl'
scaling_baseline_model: groundtruth_PPO
seed_eval: 42
env_extra_args_eval:
  init_lb: [-0.2, -0.5, 2.8, -0.3]
  init_ub: [0.2, 0.5, 3.3, 0.3]
num_eval_episodes: 10
# We take checkpoints every checkpoints_spacer checkpoints
checkpoints_spacer: 10


#################################################################################
############################ Plot desired dataset ###############################
plot_data_file: rewards_data_v2.pkl

# The percentiles used when plotting the sde distribution
alpha_percentiles: [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]
percentiles_array: [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]

# texConfig:
#     text.usetex: True
#     font.family: "serif"
#     font.serif: ["times", "times new roman"]
#     font.size: 10
#     axes.labelsize: 10
#     legend.fontsize: 10
#     xtick.labelsize: 9
#     ytick.labelsize: 9

global_legend: True

general_style:
  linewidth: 2.0

curve_plot_style:
  cartpole_bb_random_sde_PPO_gt:
    # label: "PPO tested on environment"
    # color: "#ff7f0e"
    color: "#d62728"
    color_std: "#d62728"
    zorder: 20
  cartpole_bb_random_sde_PPO:
    label: "Policy trained with learned SDE"
    color: "#2ca02c"
    color_std: "#2ca02c"
    linestyle: '--'
    zorder: 10
  gaussian_mlp_ensemble_cartpole_learned_PPO_gt:
    label: "Policy tested in environment"
    color: "#d62728"
    color_std: "#d62728"
    zorder: 20
  gaussian_mlp_ensemble_cartpole_learned_PPO:
    label: "Policy trained with learned PE"
    color: "#9467bd"
    color_std: "#9467bd"
    linestyle: '--'
    zorder: 10
  # groundtruth_PPO_gt:
  #   label: "Rew. on Groundtruth"
  #   color: "#E69F00"
  #   color_std: "#56B4E9"
  #   zorder: 30
  groundtruth_PPO:
    label: "MF PPO"
    color: "#1f77b4"
    color_std: "#1f77b4"
    linestyle: '--'
    zorder: 20
  # groundtruth_PPO_max:
  #   label: "Best agent groundtruth"
  #   color: "black"
  #   zorder: 30
  groundtruth_PPO_data:
    label: "MF PPO with $100$ trajectories"
    color: "#8c564b"
    linestyle: '-.'
    linewidth: 3
    zorder: 5

# The list of models to plot in the configuration (usually a grid) they need to be plotted
fig_args:
  figsize: [18, 5]
  nrows: 1
  ncols: 3
  sharex: True
  sharey: True
  # constrained_layout: True

extra_args:
  legend_args:
    loc: 'upper center'
    bbox_to_anchor: [0.5, 1.0]
    ncol: 3

std_style: std # minmax std perc
alpha_std: 0.5

# Groundtruth names for plotting max reward and reward at equivalent number of data
best_rew_gt: [groundtruth_PPO, ]
total_env_interact_for_training_data: 20000

plot_configs:
  - value: ['groundtruth_PPO',]
  - value: ['gaussian_mlp_ensemble_cartpole_learned_PPO', 'gaussian_mlp_ensemble_cartpole_learned_PPO_gt']
    yaxis: False
  - value: ['cartpole_bb_random_sde_PPO', 'cartpole_bb_random_sde_PPO_gt']
    yaxis: False

save_config:
  fname: 'MF_PPO_sdeVsGaussian.png'
  dpi: 500

######################################################################################
##################################### Gap plot #######################################

# global_legend: null

# curve_plot_style:
#   cartpole_bb_learned_si_sde_PPO_gt:
#     label: "With side information"
#     color: "#1f77b4"
#     color_std: "#1f77b4"
#     zorder: 20
#   cartpole_bb_learned_si_sde_PPO:
#     color: "#1f77b4"
#     color_std: "#1f77b4"
#     zorder: 20
#   cartpole_bb_learned_sde_PPO_gt:
#     label: "Without side information"
#     color: "#d62728"
#     color_std: "#d62728"
#     linestyle: '--'
#     zorder: 10
#   cartpole_bb_learned_sde_PPO:
#     color: "#d62728"
#     color_std: "#d62728"
#     linestyle: '--'
#     zorder: 10


# # The list of models to plot in the configuration (usually a grid) they need to be plotted
# fig_args:
#   figsize: [12, 6]
#   nrows: 2
#   ncols: 1
#   sharex: True

# extra_args:
#   legend_args:
#     loc: 'upper center'
#     bbox_to_anchor: [0.5, 1.0]
#     ncol: 3

# std_style: std # minmax std perc
# alpha_std: 0.3

# # Groundtruth names for plotting max reward and reward at equivalent number of data
# # best_rew_gt: [groundtruth_PPO, ]
# # total_env_interact_for_training_data: 20000

# plot_configs:
#   - value: ['cartpole_bb_learned_si_sde_PPO_gt', 'cartpole_bb_learned_sde_PPO_gt']
#     xaxis: False
#   - value: ['cartpole_bb_learned_si_sde_PPO', 'cartpole_bb_learned_sde_PPO']
#     type: gap
  
# save_config:
#   fname: 'gap_nosideinfo_vs_sideinfo.png'
#   dpi: 500

# # save_config_tex:
# #   fname: 'gap_nosideinfo_vs_sideinfo.tex'