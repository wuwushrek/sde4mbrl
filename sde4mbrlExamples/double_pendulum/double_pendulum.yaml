# Groundthruth model of the mass spring damper
# The groundtruth parameters are only used to generate data
model:

  # State, observation, and control inputs
  n_x: 4
  n_y: 4
  n_u: 1

  # This is not used when learned_nominal is not null
  init_params:
    m1: 1.0
    m2: 1.0
    l1: 1.0
    l2: 1.0
    gravity: 9.8
  
  # This parameter specified if the model should be created using groundtruth parameters
  ground_truth: False

  # The parameter specifies if control is enabled or not on the model
  control: False

  # Specify the diffusion term in the prior and posterior
  diffusion_type: constant # Zero diffusion is used when groundtruth is set to True since it is an ODE
  # diffusion_type: nonoise # This is the special diffusion when training a neural ode instead of neural sde | zero should be equivalent to this

  # Is it a black box neural ode/sde?
  include_side_info: True

  # Diffusion noise without side information
  nosi_amp_noise: [0.0001, 0.0001, 0.01, 0.01]

  # Diffusion noise with side information
  # amp_noise: [0.0001, 0.005] # 
  amp_noise: [0.0001, 0.0001, 0.01, 0.01] # 

  # ignore_diff_indx: [1] # Only consider the following noise when evaluating the kl divergence


  # Residual term enabled?
  residual_forces:
    # type: linear # quadratic, cubic,
    type: dnn
    hidden_layers: [32,]
    activation_fn: 'tanh'

  # Define the parameters for the SDE solver
  sde_solver: 'euler_maruyama'

  # Stepsize for integration
  stepsize: 0.001

  # Number of particles when sampling posterior or prior distribution
  # This can be changed at runtime
  num_particles: 1

  # Horizon of integration when sampling posterior or prior
  # This can be changed at runtime
  horizon: 1

# Loss function parameterization
sde_loss:
  seed: 1                       # The initial seed
  horizon: 20                   # The horizon to use when training the SDE -> This is constrained by what is provided by the data when the data is already given as transitions
  pen_params: 1.                # Relative importance of weight penalization with respect to kl div loss
  num_particles: 10             # Number of particles when fitting the SDE -> If different to the default model num_particles
  default_weights: 1.0e-8       # Importance of the non special parameters
  kl: 0                         # Importance of loss on the kl divergence -> already enforce in posterior
  logprob: 10.0                 # Importance on the weight of the logprob
  

sde_optimizer:
  - name: scale_by_adam
    # params:
    #   b1: 0.999
    #   b2: 0.9999
  # - name: add_decayed_weights
  #   params:
  #     weight_decay: 0.01
  - name: exponential_decay
    scheduler: True
    params:
      init_value: -0.1 # Initial learning rate (Negative value for minimization)
      transition_steps: 30000 # Basically the maximum number of gradient steps
      decay_rate: 0.0 # Typically (end_value / init_value) if end_value is expected at end
  - name: adaptive_grad_clip
    params:
      clipping: 0.01
  

# Training parameters
sde_training:
  train_batch: 1024 # The mini batch size for the training dataset
  test_batch: 1024 # The mini batch size for the training dataset
  nepochs: 100000 # The number of epochs (full pass over the training dataset)
  patience: 5000 # The number of epochs after which to stop the learningif no improvement in solution
  no_improvement_bound: 0.01 # The bound to detect no improvement in the loss
  test_freq: 50 # Number of gradient steps after which to evaluate and display losses
  save_freq: 500 # Number of gradient steps after which to save the current losses+nn
  coeff_improv_training_data: 0.0