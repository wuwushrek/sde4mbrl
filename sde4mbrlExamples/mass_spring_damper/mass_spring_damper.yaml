# XLA_PYTHON_CLIENT_MEM_FRACTION=0.5 python mass_spring_model.py --fun train --model_dir mass_spring_damper.yaml --model_type nesde_bboxes
# XLA_PYTHON_CLIENT_MEM_FRACTION=0.5 python mass_spring_model.py --fun train --model_dir mass_spring_damper.yaml --model_type nesde_bboxes --data MSD_MeasLow_TopRight_500

# Groundthruth model of the mass spring damper
# The groundtruth parameters are only used to generate data
model:

  # State, observation, and control inputs
  #[q, qdot] -> [0, 1] indexes
  n_x: 2
  n_y: 2
  n_u: 1

  # This is not used when learning the model. It provides only the groundtruth for evaluation purpose
  init_params:
    mass: 1.0
    kq_coeff: 1.0
    bqdot_coeff: 0.5
  
  # A priori knowledge of the noise distribution over the given prior dynamics knowledge
  noise_prior_params: [0.001, 0.01]
  # noise_prior_params: [0.0, 0.0] # Can be set to zero for ODEs


  # The parameter specifies if control is enabled or not on the model
  control: False

  # Is the noise term dependent on the control? Does the density needs to be estimated by merging state and control?
  control_dependent_noise: False # If not specified, False is the default value

  # Vector field unknown terms
  residual_forces:
    # type: linear # quadratic, cubic,
    type: dnn
    hidden_layers: [4, 16]
    activation_fn: 'swish'
    init_value: 0.001

  diffusion_density_nn:

    # Inputs that contribute to the unceratinty estimate
    indx_noise_in: [0,1] # If not specified, it is all the inputs

    # The components of the state where the noise is added
    # For this example, the first component dynamics are well known and not noisy
    indx_noise_out: [1,] # If not specified, it is all the state channels

    # Density scaler function -> enable to learn heteregeous noise or homogeneous noise (by commenting it)
    # In the paper, this is the W and b in 2.2: Modeling details.
    scaler_nn:
      type: scaler
      init_value: 0.01
      # activation_fn: tanh
      # hidden_layers: [4, 4]
    
    # The actual density term
    # In the paper, this is the NN_psi(x,u) in 2.2: Modeling details.
    density_nn:
      init_value: 0.1
      activation_fn: swish
      hidden_layers: [32, 32]


  # Define the parameters for the SDE solver
  sde_solver: 'euler_maruyama'

  # Stepsize for integration
  stepsize: 0.01

  # Number of particles when sampling posterior or prior distribution
  # This can be changed at runtime
  num_particles: 1

  # Horizon of integration when sampling posterior or prior
  # This can be changed at runtime
  horizon: 1


sde_loss:

  # Random seed for initializing the neural networks
  seed: 1

  # What stepsize to use when fitting the model to the data
  stepsize: 0.01

  # Number of particles when fitting the SDE -> If different to the default model num_particles
  num_particles: 1
  num_particles_test: 20 # Number of particles when doing test evaluations

  # This is the horizon of integration when computing the loss
  # It might get clipped to the horizon of each trajectory in the dataset if it's greater than trajectory length
  horizon: 50

  # Randomly subsample the horizon to fit only num_sample2consider if given
  # This simulates irreguarly sampled integration and fitting
  num_sample2consider: 10

  # Density loss parameters
  density_loss:
    learn_mucoeff:
      type: dnn
      init_value: 0.01
      activation_fn: tanh
      hidden_layers: [8, 8]
    mu_coeff: 10.0 # STrong convexity parameter
    ball_radius: 0.05 # THe radius of the ball to sample for enforcing local strong convexity
    ball_nsamples: 10 # Number of points to sample in the ball

  # Penalty for the error on the prediction
  pen_data: 1.0

  # Penalty on the gradient of the density loss
  pen_grad_density: 1.0 # 0.01

  # Penalty on the local strong convexity constraints
  pen_density_scvex: 1.0

  # Regularization penalty default value for all parameters
  pen_weights: 1.0e-6 #1.0e-6

  # Penalty on the strong convexity constant mu
  pen_mu_type: lin_inv
  pen_mu_coeff: 100.0

  # Scaling down the dad term importance relative to data
  pen_scvex_mult: 0.01


  #########################################################################
  # Extra constraints parameterization
  #########################################################################

  default_weights: 1.0
  special_parameters_pen: # Deviation from initial guess in init_params
    # We do not penalize the parameters of the diffusion terms
    scaler: 0
    density: 0

  # # When penalizing NN parameters, we want these parameters to deviate minimally from the values below
  # # The name must be a substring of the parameters defined in the model   
  # special_parameters_val:
  #   scaler: 0.0 # With zero, the scaler terms get closer to 1
  #   density: 0.0 # With zero parameters, the density term get closer to 

  # # Non specified parameters are set to
  # default_parameters_val: 0.0


sde_optimizer:
  - name: scale_by_adam
  - name: linear_schedule
    scheduler: True
    params:
      # init_value: -0.001 # Initial learning rate (Negative value for minimization)
      init_value: -0.01
      end_value: -0.001
      transition_steps: 5000 # Basically the maximum number of gradient steps 
  # - name: adaptive_grad_clip
  #   params:
  #     clipping: 0.001
  

# Training parameters
sde_training:
  train_batch: 512 # The mini batch size for the training dataset
  test_batch: 512 # The mini batch size for the training dataset
  nepochs: 100000 # The number of epochs (full pass over the training dataset)
  patience: 150 # The number of epochs after which to stop the learningif no improvement in solution
  test_freq: 50 # Number of gradient steps after which to evaluate and display losses
  save_freq: 500 # Number of gradient steps after which to save the current losses+nn
  # epochs_before_checking_improv: 10
  TestStopingCrit:
    TestErrorData: 1.0 # 0.1
    # TestStdData: -0.1 # Maximime the noise of the learned SDE
  TrainStopingCrit:
    totalLoss: 1.0
