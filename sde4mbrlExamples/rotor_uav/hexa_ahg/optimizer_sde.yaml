# XLA_PYTHON_CLIENT_MEM_FRACTION=0.5 python sde_rotor_model.py --fun train_sde --cfg hexa_ahg/optimizer_sde.yaml --out hexa_ahg_linear_nonoise
# XLA_PYTHON_CLIENT_MEM_FRACTION=0.5 python sde_rotor_model.py --fun gen_traj --cfg hexa_ahg/data_generation.yaml

# The directory containing my_models and my_data
vehicle_dir: ~/Documents/sde4mbrl/sde4mbrlExamples/rotor_uav/hexa_ahg

# Training and testing data trajectory
data_dir: trajs.pkl

# # If few parameters were learnt using static matching of model of vector field and finite difference, we specify its name here
learned_nominal: prior_hexa_lin.yaml # a name of a learned .yaml file -> Generate this file using optimizer_prior or comment it if you don't want to learn a prior

# Model parameters
model:
  # x: 0, y: 1, z: 2, vx: 3, vy: 4, vz: 5, qw: 6, qx: 7, qy: 8, qz: 9, wx: 10, wy: 11, wz: 12
  n_x: 13
  n_y: 13
  n_u: 6

  # A priori knowledge of the noise distribution over the given prior dynamics knowledge
  # noise_prior_params: [0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.0001, 0.0001, 0.0001, 0.0001, 0.01, 0.01, 0.01]
  noise_prior_params: [0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.05, 0.05, 0.05]
  # noise_prior_params: [0., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

  # This is not used when learned_nominal is not null
  # This means that, when learned_nominal is null, these values are used to initialize the corresponding parameters in the model.
  # Those parameters are later trained on the data
  # When learned_nominal is not null, then it is .yaml file in my_models that contains static lerned parameters
  # This parameters are definitely not accurate except for the mass which was obtained in Gazebo
  # Prior model parameter path
  init_params:
    mass: 2.295
    gravity: 9.81
    # Quick estimate of the intertia based on the frame geometry
    Ixx: 0.008
    Iyy: 0.008
    Izz: 0.011
    fm_model:
      order: 2
      use_constant_term: True
      assume_sym: True

  # This enforce that the init_params given by learned_nominal are fixed and not relearned
  fixed_init_params: False # Set this value to False if you want to relearn it. This value is ignored or set to Fals if learned_nominal is not given

  # Is the noise term dependent on the control? Does the density needs to be estimated by merging state and control?
  control_dependent_noise: False # If not specified, False is the default value

  # Scaling factor for normalizing neural network inputs
  # x: 0, y: 1, z: 2, vx: 3, vy: 4, vz: 5, qw: 6, qx: 7, qy: 8, qz: 9, wx: 10, wy: 11, wz: 12
  state_scaling: [3.5, 2.5, 2.9, 1.5, 1.9, 1.9, 1, 0.25, 0.25, 1, 4, 3.1, 2.2]

  diffusion_density_nn:
    # All inputs are contributing to the noise
    # Inputs that contribute to the noise estimation
    indx_noise_in: [3, 4, 5, 10, 11, 12] # If not specified, it is all the inputs

    # The learned noise only operates on this set of outputs
    indx_noise_out: [3, 4, 5, 10, 11, 12] # If not specified, it is all the putputs

    # Density scaler function -> learn heteregeous noise or homogeneous noise with none type
    scaler_nn:
      type: scaler
      init_value: 0.01
      # activation_fn: tanh
      # hidden_layers: [4, 4]
    
    density_nn:
      init_value: 0.001
      activation_fn: tanh
      hidden_layers: [16, 16]
      aggressiveness: 0.75

  # Ground effect learning enabled?
  ground_effect: False # True -> tb

  # Aerodynamic drag enabled?
  aero_drag_effect: True

  # Residual term enabled?
  residual_forces:
    #vb_x, vb_y, vb_z, wx, wy, wz 
    active_indx: [0, 1, 2]
    hidden_layers: [8, 16]
    activation_fn: 'tanh'

  residual_moments:
    #vb_x, vb_y, vb_z, wx, wy, wz 
    # active_indx: [0, 1, 2] If not specified, then everything is used
    hidden_layers: [8, 16]
    activation_fn: 'tanh'

  # Project some of the parameters on the nonnegative domain
  noneg_params: ['aero_kdx', 'aero_kdy', 'aero_kdz', 'aero_kh', 'mass', 'Ixx', 'Iyy', 'Izz']

  # Define the parameters for the SDE solver
  sde_solver: 'euler_maruyama'

  # Stepsize for integration
  stepsize: 0.01

  # Number of particles when sampling posterior or prior distribution
  # This can be changed at runtime
  num_particles: 1

  # Horizon of integration when sampling posterior or prior
  # This can be changed at runtime
  horizon: 1

sde_loss:

  # Random seed for initializing the neural networks
  seed: 1

  # What stepsize to use when fitting the model to the data
  data_stepsize: 0.01
  stepsize: 0.05
  u_sampling_strategy: random

  # Number of particles when fitting the SDE -> If different to the default model num_particles
  num_particles: 1
  num_particles_test: 5

  # This is the horizon of integration when computing the loss
  # It might get clipped to the horizon of each trajectory in the dataset if it's greater than trajectory length
  # horizon: 500
  horizon: 20

  # Randomly subsample the horizon to fit only num_sample2consider if given
  num_sample2consider: 5
  # How the states are scaled in the loss computation
  # obs_weight: [5, 5, 5, 1.5, 1.9, 1.9, 8, 8, 8, 8, 4, 3.1, 2.2]
  obs_weights: [3.5, 2.5, 2.9, 1.5, 1.9, 1.9, 1, 0.25, 0.25, 1, 7, 7, 7]
  # obs_weight: [3.2, 2.4, 2.9, 1.5, 1.9, 1.9, 1, 1, 1, 1, 4, 3, 1.7]

  # Density loss parameters
  density_loss:
    learn_mucoeff: False # Should we learn the local strong convexity parameters?
    mu_coeff: 10.0 # STrong convexity parameter
    ball_radius: 0.05 # THe radius of the ball to sample for enforcing local strong convexity
    ball_nsamples: 10 # Number of points to sample in the ball
  
  # Penalty for the error on the prediction
  pen_data: 1.0

  # Penalty on the gradient of the density loss
  pen_grad_density: 1.0 # 0.01

  # Penalty on the local strong convexity constraints
  pen_density_scvex: 1.0

  # Regularization penalty default value for all parameters
  pen_weights: 1.0e-3

  #########################################################################
  # Extra constraints parameterization
  #########################################################################

  default_weights: 1.0

  special_parameters_pen: # Deviation from initial guess in init_params
    scaler: 0
    density: 0
    # aero_kdx: 1.0e-5
    # aero_kdy: 1.0e-5
    # aero_kdz: 1.0e-5
    # aero_kh: 1.0e-5
    # mass: 10000000
    # Ixx: 2.0
    # Iyy: 2.0
    # Izz: 2.0
  
  # special_parameters_pen: # Deviation from initial guess in init_params
  #   # aero_kdx: 1.0e-5
  #   # aero_kdy: 1.0e-5
  #   # aero_kdz: 1.0e-5
  #   # aero_kh: 1.0e-5
  #   mass: 2.0
  #   Ixx: 2.0
  #   Iyy: 2.0
  #   Izz: 2.0
  # nonneg_nonzero:
  #   Ixx: 1.0e-4


sde_optimizer:
  - name: scale_by_adam
    # params:
    #   b1: 0.999
    #   b2: 0.9999
  # - name: add_decayed_weights
  #   params:
  #     weight_decay: 0.001
  # - name: linear_schedule
  #   scheduler: True
  #   params:
  #     init_value: -0.01 # Initial learning rate (Negative value for minimization)
  #     end_value: -0.001
  #     transition_steps: 30000 # Basically the maximum number of gradient steps 
  - name: piecewise_constant_schedule
    scheduler: True
    params:
      init_value: -0.001 # Initial learning rate (Negative value for minimization)
      boundaries_and_scales:
        4000: 10.0
        18000: 0.5
        30000: 0.2
  

# Training parameters
sde_training:
  train_batch: 128 # The mini batch size for the training dataset
  test_batch: 512 # The mini batch size for the training dataset
  nepochs: 5000 # The number of epochs (full pass over the training dataset)
  patience: 2000 # The number of epochs after which to stop the learningif no improvement in solution
  test_freq: 50 # Number of gradient steps after which to evaluate and display losses
  save_freq: 500 # Number of gradient steps after which to save the current losses+nn
  # Additional parameters to display when showing loss evolution
  show_param: ['aero_kdx', 'aero_kdy', 'aero_kdz', 'aero_kh',  'mass', 'Ixx', 'Iyy', 'Izz']
  TestStopingCrit:
    TestErrorData: 1.0 # 0.1
    # TestStdData: -0.1 # Maximime the noise of the learned SDE
  TrainStopingCrit:
    totalLoss: 1.0
