# XLA_PYTHON_CLIENT_MEM_FRACTION=0.5 python train_static_model.py --cfg hexa_sitl/optimizer_prior.yaml --out prior_hexa

# An example of config file used to train the geometric parameters of the quadcopter
# along with the thurst and moment response to motors inputs
n_x : 13
n_u : 6
n_y : 13


# Path the the log data
logs_dir:
  - ~/Documents/log_flights/hexa_sitl_1.ulg
  - ~/Documents/log_flights/hexa_sitl_2.ulg
  - ~/Documents/log_flights/hexa_sitl_3.ulg
  # More trajectories if available

# Testing trajectory
test_trajectory: ~/Documents/log_flights/hexa_sitl_3.ulg

non_neg_params: ['Ixx', 'Iyy', 'Izz', 'mass']

# Seed for random number genertaion
seed: 0

# Directory of prior model file. This is where the outputs are going to be saved
vehicle_dir: ~/Documents/sde4mbrl/sde4mbrlExamples/rotor_uav/hexa_sitl

# Prior model parameter path
init_params:
  mass: 2.02
  gravity: 9.81
  Ixx: 0.011
  Iyy: 0.015
  Izz: 0.021
  fm_model:
    order: 1
    use_constant_term: True
    assume_sym: False
    learn_mixer: False

# Enforce that the trajectory log is filtered every time
# When false, the trajectory is only filtered the first time and save on the disk
force_filtering: False

# Cutoff frequency for filtering
# The details on how to find them can be seen in filter_data_analysis.ipynb
cutoff_freqs:
  x: 2
  y: 2
  z: 2
  vx: 4
  vy: 4
  vz: 4
  qw: 6
  qx: 6
  qy: 6
  qz: 6
  wx: 6
  wy: 6
  wz: 6

# Loss function parameters
# Parameters to weight objective in the loss function
loss:
  pen_xdot: 1.0                # Weight coefficient on the vector field
  pen_params: 1.0               # Weight on all the params of the problem
  default_weights: 0         # The default weight penalty terms
  special_parameters_pen: # Penalty coefficient for special parameters
    mass: 1.
    # Ixx: 10.0
    # Iyy: 10.0
    # Izz: 10.0

# Optimizer parameters
optimizer:
  - name: scale_by_adam
    # params:
    #   b1: 0.999
    #   b2: 0.9999
  - name: linear_schedule
    scheduler: True
    params:
      init_value: -0.01 # Initial learning rate (Negative value for minimization)
      end_value: -0.001
      transition_steps: 50000 # Basically the maximum number of gradient steps 
  # - name: exponential_decay
  #   scheduler: True
  #   params:
  #     init_value: -0.01 # Initial learning rate (Negative value for minimization)
  #     transition_steps: 80000 # Basically the maximum number of gradient steps
  #     decay_rate: 0.0 # Typically (end_value / init_value) if end_value is expected at end
  # - name: adaptive_grad_clip
  #   params:
  #     clipping: 0.001

# Training parameters
training:
  train_batch_size: 4096  # The mini batch size for the training dataset
  test_batch_size: 1024       # The mini batch used for evaluating the training trajectory

  nepochs: 100000             # The number of epochs (full pass over the training dataset)
  
  patience: 40000             # The number of epochs after which to stop the learningif no improvement in solution

  test_freq: 100              # Number of gradient steps after which to print the loss and all
  save_freq: 500              # Number of gradient steps after which to save the current losses+nn
  display_freq: 2000          # Number of gradient steps after which we lot the current solution

  key_to_show: null           # The parameters to print during the loss

  display_losses: True        # Display the losses as the training evolves
  TrainCoeff: 1.0
